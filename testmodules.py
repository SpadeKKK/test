# test all modules

import os
import numpy as np
from scipy import stats
import itertools
import pandas as pd
import pymzml
import pickle
import matplotlib.pyplot as plt
import math
#os.chdir("/Users/duranbao/Desktop/Study_abroad/PhD_projects/ML_update/ML_project/")
os.chdir("C:\\Users\\hanyu\\Documents\\Duran\\Pre_AI\\20200313_new_anno\\ML_project") # identify the working directionary

# In[calculate features from raw data]
from extraction.fe_cal import *
# generate pickle files of features for each dataset
#fe_cal('MLIT_T')
#fe_cal('MLIT_V')
#fe_cal('INDTS1')
#fe_cal('INDTS2')

# In[visualize data]
from plots.visual import *
import matplotlib.pyplot as plt
# visual_all('INDTS2') # this step could visualize all figures into html file and compare them together
# Notice: This step should use the original mzml data to draw figures instead of pickle file.

filenames = "MLIT_T"
sample = "MLTR0003"

# positive: MLTR0003, negative: MLTR0005
# Now we need select several demos to show their chromatographies
fig_i, fig_t= visual(filenames, sample)
fig_ni, fig_nt = visualnor(filenames, sample) # visualize samples by normalized
fig_ti, fig_tt = visualsum(filenames, sample) # visualize samples by integrated intensity
fig_tni, fig_tnt = visualsumnor(filenames, sample) # visualize data

#fig_nt.set_size_inches(8, 4)
#fig_nt.savefig('peakwithbd.png', dpi = 200)  # save original figure locally
# In[input features data]
from dataset.dataset import *
from extraction.features import *
suffix = '_MLIT_T'  
info_file = f"data{suffix}/data{suffix}.csv" # change the filename
y = dataset(info_file)
datafile_name = f"data{suffix}/saved_data{suffix}.pkl"
with open(datafile_name, 'rb') as f:
    data = pickle.load(f)

fe_lst = ['s', 'm', 'sm']
X_temp = []
for i in fe_lst:
    X_temp.append(diffeature(data, i)) # Extract sta, mor, sta&mor features

# In[train algorithms with different categorical features, this part run ~45 min]
from models.oritrain import ori2
import time
start_time = time.time() # started time 

fe_lst = ['s', 'm', 'sm']
blr_lst = [] # best logistic regression algorithm number
brf_lst = [] # best random forest algorithm number
bgb_lst = [] # best gradient boosting algorithm number
k = 0
for i in fe_lst:
    lr, rf, gb = ori2(X_temp[k], y, i, 5)  # X_temp was list formed in previous steps
    blr_lst.append(lr)
    brf_lst.append(rf)
    bgb_lst.append(gb)
    k += 1
# Different categorical features will generate and save into corresponding folder.
print("--- %s seconds ---" % (time.time() - start_time)) # measure how long will take in this section

# In[test the best combination by valid, independent datasets]

from dataset.dataset import *
from extraction.features import *
from models.validate import *
'''
# Next lists are already generated by previous step, we could use them directly.
blr = [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2]]
brf = [[14, 9, 12, 8, 14], [13, 9, 5, 13, 9], [11, 11, 5, 5, 8]]
bgb = [[16, 16, 16, 16, 16], [16, 16, 16, 16, 16], [16, 16, 16, 16, 16]]
'''
suffix_lst = ["MLIT_V", "INDTS1", "INDTS2"]
for suffix in suffix_lst:
    acc_values = []
    logloss_values = []
    sen_values = []
    auc_values = []
    fe_lst = ['s', 'm', 'sm']
    for k in fe_lst:
        for j in range(5):
            info_file = f"data_{suffix}/data_{suffix}.csv" 
            label_list = dataset(info_file)
            with open(f"data_{suffix}/saved_data_{suffix}.pkl", 'rb') as f:
                data = pickle.load(f)
            i = 0
            Xval = diffeature(data, k)
            yval = label_list
            acc, ll, sen, auc = validate(Xval, yval, k, blr_lst[i][j], brf_lst[i][j], bgb_lst[i][j])  # change the feature category blr[i], brf[i], bgb[i]
            acc_values.append(acc)
            logloss_values.append(ll)
            sen_values.append(sen)
            auc_values.append(auc)
            i +=1
    accdf = pd.DataFrame(acc_values, columns = ['LR', 'RF', 'GB'])
    accdf['feature'] = ['s']*5 + ['m']*5 + ['sm']*5
    accdf['random'] = [0, 1, 2, 3, 4]*3
    accdf.to_csv(f"{suffix}_Accuracy.csv")

    lldf = pd.DataFrame(logloss_values, columns = ['LR', 'RF', 'GB'])
    lldf['feature'] = ['s']*5 + ['m']*5 + ['sm']*5
    lldf['random'] = [0, 1, 2, 3, 4]*3
    lldf.to_csv(f"{suffix}_Logloss.csv")

    sendf = pd.DataFrame(sen_values, columns = ['LR', 'RF', 'GB'])
    sendf['feature'] = ['s']*5 + ['m']*5 + ['sm']*5
    sendf['random'] = [0, 1, 2, 3, 4]*3
    sendf.to_csv(f"{suffix}_Sencitivity.csv")
    
    aucdf = pd.DataFrame(auc_values, columns = ['LR', 'RF', 'GB'])
    aucdf['feature'] = ['s']*5 + ['m']*5 + ['sm']*5
    aucdf['random'] = [0, 1, 2, 3, 4]*3
    aucdf.to_csv(f"{suffix}_AUC.csv")

# In[make ROC curves for train and test dataset]
from plots.roc import *

suffix_lst = ["MLIT_V", "INDTS1", "INDTS2"]
fe_lst = ['s', 'm', 'sm']

for i in suffix_lst:
    for j in fe_lst:
        for k in range(3):
            X_train = X_temp[k]
            y_train = y
            X_test = difffeature(i, j)
            y_test = dataset(i, j)
            fig = roc(X_train, y_train, X_test, y_test)
            # fig.savefig(fname="roc{i}_{j}.png")

# In[output feature tables]
from features.fetable import *
newX = fetable(X_train, "sm") # "s", "m", "sm"
newX['label'] = y
#newX.to_csv(f"data{suffix}/total_features.csv", index=False)

# In[split training dataset into 10 fold]
from sklearn.model_selection import train_test_split
X_trainset = []
y_trainset = []
X_testset = []
y_testset = []
for i in range(10):
    if i < 9: 
        a,b,c,d = train_test_split(X, y, train_size = 0.1*(i+1), random_state = 0, stratify = y)
        X_trainset.append(a)
        X_testset.append(b)
        y_trainset.append(c)
        y_testset.append(d)
    else:
        X_trainset.append(X)
        y_trainset.append(y)



# In[shapvalue]
import shap
import os
import matplotlib.pyplot as pl
from shapvalues.shaptable import *
X['label'] = y
shpdf, clf = shaptable(X) #output is the shap values by features, and the corresponding algorithm
shpdf.to_csv("shpdf1.csv") # output shap values table

#shap figure
explainer = shap.TreeExplainer(clf)
shap_values = explainer.shap_values(X.drop('label',1))
shap.summary_plot(shap_values[1], X.drop('label',1),max_display=15,show=False)
f=pl.gcf() # shap values figure
f.savefig("feature_imp.svg",format='svg') # save the figure into high quality or other format



